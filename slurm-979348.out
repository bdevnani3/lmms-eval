Running baseline for mvbench saving results at /data/jjain45/bhavika/results/llava_onevision/mvbench/baseline
/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
There was a problem when trying to write in your cache folder (/data/jjain45/bhavika). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
There was a problem when trying to write in your cache folder (/data/jjain45/bhavika). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.
[32m2025-05-05 02:19:51.444[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m332[0m - [1mVerbosity set to INFO[0m
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[32m2025-05-05 02:19:51.758[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2025-05-05 02:19:51.761[0m | [33m[1mWARNING [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m473[0m - [33m[1mThe tag coco_karpathy is already registered as a group, this tag will not be registered. This may affect tasks you want to call.[0m
[32m2025-05-05 02:19:54.261[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m415[0m - [1mEvaluation tracker args: {'output_path': '/data/jjain45/bhavika/results/llava_onevision/mvbench/baseline'}[0m
[32m2025-05-05 02:19:54.262[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m504[0m - [1mSelected Tasks: ['mvbench'][0m
*********Temporal aggregation:  None
*********im_resize_shape:  16
*********max_frames_num:  32
*********DEBUG MODE************  False
[32m2025-05-05 02:19:54.269[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m155[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Traceback (most recent call last):
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/tenacity/__init__.py", line 470, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/api/task.py", line 946, in download
    cache_path = snapshot_download(repo_id=self.DATASET_PATH, revision=revision, repo_type="dataset", force_download=force_download, etag_timeout=60)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py", line 265, in snapshot_download
    os.makedirs(os.path.dirname(ref_path), exist_ok=True)
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 215, in makedirs
  File "<frozen os>", line 215, in makedirs
  [Previous line repeated 2 more times]
  File "<frozen os>", line 225, in makedirs
PermissionError: [Errno 13] Permission denied: '/data'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/__main__.py", line 367, in cli_evaluate
    results, samples = cli_evaluate_single(args)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/__main__.py", line 512, in cli_evaluate_single
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/utils.py", line 533, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/evaluator.py", line 171, in simple_evaluate
    task_dict = get_task_dict(tasks, task_manager)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/tasks/__init__.py", line 558, in get_task_dict
    task_name_from_string_dict = task_manager.load_task_or_group(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/tasks/__init__.py", line 372, in load_task_or_group
    all_loaded_tasks = dict(collections.ChainMap(*map(self._load_individual_task_or_group, task_list)))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/tasks/__init__.py", line 358, in _load_individual_task_or_group
    return {group_name: dict(collections.ChainMap(*map(fn, reversed(subtask_list))))}
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/tasks/__init__.py", line 289, in _load_individual_task_or_group
    return _load_task(task_config, task=name_or_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/tasks/__init__.py", line 259, in _load_task
    task_object = ConfigurableTask(config=config, model_name=self.model_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/final_lmms_eval/lmms-eval/lmms_eval/api/task.py", line 720, in __init__
    self.download(self.config.dataset_kwargs)
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/tenacity/__init__.py", line 330, in wrapped_f
    return self(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/tenacity/__init__.py", line 467, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/tenacity/__init__.py", line 368, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/coc/flash9/bdevnani3/anaconda/lib/python3.12/site-packages/tenacity/__init__.py", line 411, in exc_check
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7f5eef8a1ca0 state=finished raised PermissionError>]
[32m2025-05-05 02:20:02.987[0m | [31m[1mERROR   [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m386[0m - [31m[1mError during evaluation: RetryError[<Future at 0x7f5eef8a1ca0 state=finished raised PermissionError>]. Please set `--verbosity=DEBUG` to get more information.[0m
